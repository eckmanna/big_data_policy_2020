{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "pd.options.display.max_columns = None # Display all columns of a dataframe\n",
    "pd.options.display.max_rows = 700\n",
    "from pprint import pprint\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 2: Working with Big Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "## Motivation\n",
    "\n",
    "**Publication of crawling papers by year**\n",
    "\n",
    "![Publication of crawling papers by year](images/publication_crawling_papers_by_year.png)*Source*: Claussen, Jörg and Peukert, Christian, **Obtaining Data from the Internet: A Guide to Data Crawling in Management Research** (June 2019). Available at SSRN: https://ssrn.com/abstract=3403799 or http://dx.doi.org/10.2139/ssrn.3403799\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**General objective of the notebook**: construct a dataset with the **tweets** of the current U.S. members of Congress (Senate + House) with information on their party **affiliation**\n",
    "\n",
    "**Three sources of data**:\n",
    " 1. **List of U.S. representatives**: **webscraped** from [ballotpedia](https://ballotpedia.org/List_of_current_members_of_the_U.S._Congress)\n",
    " 2. **Twitter accounts** of the U.S. representative. From a [hand-labeld dataset](https://github.com/vegetable68/Midterm-2018-candidates) compiled by Yiqing Hua for all candidates.\n",
    " 3. **Tweets** published on the twitter accounts\n",
    " \n",
    "**2 merge operations**:\n",
    "- 1+2: select only the elected representative among the candidates present in 2\n",
    "- 3+2+1: tweets associated with their author + party affiliation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Screen scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is webscraping ?\n",
    "\n",
    "<img src=\"images/screenscraping.png\">\n",
    "\n",
    "Source: [SICSS](https://compsocialscience.github.io) \n",
    "\n",
    "Points to keep in mind:\n",
    "- It may or may not be legal\n",
    "- Webscraping is tedious and frustrating\n",
    "\n",
    "Main challenges:\n",
    "- Variety of websites and webpages\n",
    "- Durability of code as website constantly changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Typical Steps of Webscraping\n",
    "\n",
    "### Exploring the Website\n",
    "\n",
    "We will scrape the list of current members of the U.S. Congress because it will be useful later in the class!\n",
    "<img src=\"images/ballotpedia.png\">\n",
    "\n",
    "Source: [ballotpedia website](https://ballotpedia.org/List_of_current_members_of_the_U.S._Congress) \n",
    "\n",
    "### Understanding URLs\n",
    "- Base URL: https://ballotpedia.org/List_of_current_members_of_the_U.S._Congress\n",
    "- More complex URL with query parameter https://ballotpedia.org/wiki/index.php?search=jerry&searchToken=elnan6bftyqukadgu8xb2rtbg\n",
    "    - query parameter=`p?search=jerry`\n",
    "    - can be used to crawl websites if you have a list of queries that you want to loop over (e.g. dates, localities...)\n",
    "    - query structure:\n",
    "        - *Start*: `?`\n",
    "        - *Information*: pieces of information constituting one query parameter are encoded in key-value pairs, where related keys and values are joined together by an equals sign (key=value). \n",
    "        - *Separator*: `&` -> if multiple query parameters \n",
    "        \n",
    "Other example of URL: https://opendata.swiss/en/dataset?political_level=commune&q=health. Try to change the search and selection parameters and observe how that affects your URL. \n",
    "\n",
    "### Inspect the site Using Developer Tools\n",
    "We use the `inspect` function (right click) to access the underlying HTML interactively. \n",
    "<img src=\"images/ballotpedia_inspect.png\">\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R users** \n",
    "\n",
    "The logic shown hereafter has its direct equivalent in `R`. See [this post](https://towardsdatascience.com/web-scraping-tutorial-in-r-5e71fd107f32) for examples of the most useful functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HTML parsing\n",
    "In this example, we scrape **static HTML content**: the server that hosts the site sends back HTML documents that already contain all the data you’ll get to see as a user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib # Python's module for accessing web pages\n",
    "url='https://ballotpedia.org/List_of_current_members_of_the_U.S._Congress'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = urllib.request.urlopen(url) # open the web page\n",
    "html = page.read() # read web page contents as a string\n",
    "print(\"-- first 400 characters --\", html[:400]) \n",
    "print(\"-- last 400 characters --\", html[-400:])\n",
    "print(\"-- length of string --\", len(html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse raw HTML\n",
    "from bs4 import BeautifulSoup # package for parsing HTML\n",
    "soup = BeautifulSoup(html, 'html.parser') # parse html of web page\n",
    "print(\"-- title item:\", soup.title) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract text\n",
    "text = soup.get_text() # get text (remove HTML markup)\n",
    "lines = text.splitlines() # split string into separate lines\n",
    "print(\"-- Number of lines:\", len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = [line for line in lines if line != ''] # drop empty lines\n",
    "print(\"-- Number of lines (after dropping empty lines):\", len(lines))\n",
    "print(\"-- The first 20 lines:\", lines[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping a table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find Elements by ID\n",
    "using the `find` function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.find(id=\"mw-content-text\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=soup.find(id=\"mw-content-text\")\n",
    "print(results.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find Elements by HTML Class Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find('table', class_='wikitable sortable jquery-tablesorter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`find_all` is often more useful than `find`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = soup.find_all('table', class_='wikitable sortable jquery-tablesorter')\n",
    "len(tb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "senate=tb[0] # first element \n",
    "print(senate.find_all('tr')[1]) # a row of the table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract Text From HTML Elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re # for regular expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning the row: usnig the `get_text` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row=senate.find_all('tr')[1]\n",
    "\n",
    "print(\"-- Number of rows: {}\".format(len(senate.find_all('tr'))))\n",
    "\n",
    "# Using the a of the first 2 cells\n",
    "for cell in row.find_all('a'):\n",
    "    print(cell.get_text())\n",
    "\n",
    "# For the 2 last cells:\n",
    "for cell in row.find_all('p')[2:4]:\n",
    "    print(re.sub('\\n', '', cell.get_text().lstrip())) # little text trick: wait for the class on text-as-data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop over all rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_senate=pd.DataFrame() # empty dataframe in which the cleaned rows will be stored\n",
    "\n",
    "for row in senate.find_all('tr'):\n",
    "    row_dict=dict() # empty dictionary in which the cleaned cells are stored\n",
    "    i=0\n",
    "    # Using the a of the first 2 cells\n",
    "    for cell in row.find_all('a'):\n",
    "        row_dict[i]=[cell.get_text()]\n",
    "        i=i+1\n",
    "    # For the 2 last cells:\n",
    "    for cell in row.find_all('p')[2:4]:\n",
    "        row_dict[i]=[re.sub('\\n', '', cell.get_text().lstrip())]\n",
    "        i=i+1\n",
    "    df_row=pd.DataFrame.from_dict(row_dict, orient='columns') # row_dict -> dataframe\n",
    "    df_senate=pd.concat([df_senate, df_row]) # append the df_row "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_senate=df_senate.rename(columns={0:'Officeholder name', 1: 'Office title', 2: 'Date assumed office', 3: 'Party affiliation'})\n",
    "df_senate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_senate['Party affiliation'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: construct a dataframe containing the table on the House "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The house composition is the second table of the page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Going further\n",
    "\n",
    "There are also **dynamic websites**: the server does not always send back HTML, but your browser also receive and interpret JavaScript code that you cannot retreive from the HTML. You receive JavaScript code that you cannot parse using `beautiful soup` but that you would need to execute like a browser does. \n",
    "\n",
    "Solutions: \n",
    "- Use `requests-html` \n",
    "- Simulate a browser using [selenium](https://selenium-python.readthedocs.io/) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data on politician with info on party and twitter accounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to find (or build from scratch) a data with information on the politician. Most importantly, we need a link to their twitter account and their party affiliation. \n",
    "\n",
    "Such a dataset has been constructed by Yiqing Hua (Cornell Tech) for  US. midterm election 2018 candidates with their twitter handles data from https://github.com/vegetable68/Midterm-2018-candidates\n",
    "\n",
    "Data = full list of candidates running for House and Senate, as well as gubernatorial candidates from Ballotpedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read file with pandas (stored on github)\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/vegetable68/Midterm-2018-candidates/master/candidates.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['party'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['gender'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_candidates=df[['candidate_name', 'party', 'twitter handle']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge with House and Senate data : only keeps the elected candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_congress= pd.concat([df_house, df_senate])\n",
    "df_congress.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_candidates.shape)\n",
    "print(\"Number of unique candidate names\", len(df_candidates['candidate_name'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_all=pd.merge(df_congress, df_candidates, right_on='candidate_name', left_on='Officeholder name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--Result of the merge:\")\n",
    "print(\"Number of twitter accounts from candidates:\", df_candidates.shape[0])\n",
    "print(\"Number of twitter accounts from US representative:\", df_merged_all.shape[0])\n",
    "print(\"Correspond to {} politicians (often having 2 accounts)\".format(len(df_merged_all['Officeholder name'].unique())))\n",
    "print(\"Number US representative:\", df_congress.shape[0])\n",
    "print(\"Share of US representative with a twitter account:\", len(df_merged_all['Officeholder name'].unique())/df_congress.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of tweeter accounts, useful for the following task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "account_list = df_merged_all['twitter handle'].tolist()\n",
    "print('First 3 elements:', account_list[:3])\n",
    "print('Number of twitter account studied:', len(account_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application Programming Interfaces (API)\n",
    "### What Is an API?\n",
    "\n",
    "**APIs are tools for building apps or other forms of software that help people access certain parts of large databases**\n",
    "\n",
    "The website [Programmable Web](https://www.programmableweb.com/apis/directory) lists more than 225,353 API from sites as diverse as Google, Amazon, YouTube, the New York Times, del.icio.us, LinkedIn, and many others.\n",
    "\n",
    "<img src=\"images/growth_in_web_api.png\">\n",
    "\n",
    "Source: [Programmable Web](https://www.programmableweb.com/news/apis-show-faster-growth-rate-2019-previous-years/research/2019/07/17) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Does an API Work?\n",
    "\n",
    "Better than webscraping if possible because: \n",
    "- More stable than webpages\n",
    "- No HTML but already structured data (e.g. in `json`)\n",
    "\n",
    "### API Credentials\n",
    "In order to prevent software developer to collect huge amount of individual data, many APIs require you to obtain “credentials” or codes/passwords that identify you and determine which types of data you are allowed to access. \n",
    "\n",
    "### Rate Limiting\n",
    "The credentials not only define what type of information we are allowed to access, but also how often we are allowed to make requests for such data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Using Twitter's API?\n",
    "\n",
    "- Increasingly used in Political Sciences and Economics \n",
    "    - Allyson L. Benton & Andrew Q. Philips, 2020. **\"Does the @realDonaldTrump Really Matter to Financial Markets?,\"** *American Journal of Political Science*, John Wiley & Sons, vol. 64(1), pages 169-190, January. [Website](https://onlinelibrary.wiley.com/doi/10.1111/ajps.12491)\n",
    "    - Petrova Maria Sen Ananya and Yildirim Pinar, **Social Media and Political Donations: New Technology and Incumbency Advantage in the United States** (September 8, 2016). [SSRN](https://ssrn.com/abstract=2836323)\n",
    "    - **Analyzing Polarization in Social Media: Method and Application to Tweets on 21 Mass Shootings** by Dorottya Demszky, Nikhil Garg, Matthew Gentzkow,  Rob Voigt, James Zou, Jesse M. Shapiro, and Dan Jurafsky, 17th Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL). June 2019. [arxiv](https://arxiv.org/abs/1904.01596)\n",
    "- As an example for using an API\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Example with Twitter’s API\n",
    "\n",
    "### How to apply for a developer account\n",
    "Developers need first to have a twitter account: this tutorial assumes that it is already the case. \n",
    "\n",
    "How to obtain credentials from Twitter that will allow you to make API calls? \n",
    "\n",
    "1. create an account (https://apps.twitter.com) in order to receive credentials  \n",
    "2. create a developer account by clicking ''Apply for a developer account''. \n",
    "3. confirm your email address or add a mobile phone number (two-factor authentication helps Twitter prevent people from obtaining a large number of different credentials using multiple accounts that could be use to collect large amounts of data without being rate limited—or, for other nefarious purposes such as creating armies of bots that produce spam or attempt to influence elections.)\n",
    "4. answer series of questions about how you want to use Twitter’s API & accept terms of services\n",
    "5. Once you accept the terms, your app developer request will go under review by Twitter. Then it takes time (1-2 days to a week)\n",
    "\n",
    "### Create an Application & get your authentification details\n",
    "1. Once the developer account is approved, go to your profile tab and select Apps. Create an app and fill in the details.\n",
    "2. Click on `details`\n",
    "3. Click on `Keys and tokens`. This is where you get the relevant keys (you will have to regenerate and copy the tokens):\n",
    "    - API key\n",
    "    - API secret key\n",
    "    - Access token\n",
    "    - Access token secret\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After registering to the Twitter API, you get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #this you get when you make create an application on twitter as a dev\n",
    "consumer_key=\"YOURKEYHERE\"\n",
    "consumer_secret=\"YOURSECRETHERE\"\n",
    "access_token=\"YOURACCESSTOKENHERE\"\n",
    "access_token_secret=\"YOURACCESSTOKENSECRETHERE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing the Twitter API using `tweepy`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `tweepy` package (documentation: https://tweepy.readthedocs.io/en/latest/). Tweepy is an *An easy-to-use Python library for accessing the Twitter API.*\n",
    "\n",
    "R users can use [rtweet](https://rtweet.info/), a similar package. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Twitter requires all requests to use `OAuth` for authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from tweepy import OAuthHandler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authenticate to Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = OAuthHandler(consumer_key, consumer_secret) #creating an OAuthHandler instance\n",
    "auth.set_access_token(access_token, access_secret)\n",
    "\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We specify `wait_on_rate_limit_notify==True` & `wait_on_rate_limit=True`. The API method will wait once you’ve reached your rate limit and prints out a message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test authentication\n",
    "try:\n",
    "    api.verify_credentials()\n",
    "    print(\"Authentication OK\")\n",
    "except:\n",
    "    print(\"Error during authentication\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an extensive list of the methods available, see the [API Reference page](https://tweepy.readthedocs.io/en/latest/api.html#api-reference). There are several types of methods. The following methods enable you to access twitter content:\n",
    "- Timeline methods return a list of  `status` objects\n",
    "- Status methods return a `status` object\n",
    "- User methods return `user` object or a list of `user` objects. \n",
    "- Favorite methods: return a list of  `status` objects\n",
    "\n",
    "For some methods, you can interact with twitter:\n",
    "- Friendship Methods return a `user` object, by example:\n",
    "    - `create_friendship`: creates a new friendship with the specified user ()\n",
    "    \n",
    "Let's review some useful methods:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods returning a `status` object (or a list of objects)\n",
    "#### Search method\n",
    "\n",
    "If you seeking Twitter data to get conversations on a particular topic. This method returns a collection of relevant Tweets matching a specified query for all public tweets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most recent tweets about ETH \n",
    "tweets = api.search(q=\"ETH Zürich\", lang=\"en\")\n",
    "for tweet in tweets:\n",
    "    print(tweet.text) # printing the first tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `status` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(tweets[0]) # for the first tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(tweets[0]._api) # for the first tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(tweets[0]._json) # for the first tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Small introduction to `json` format and dictionaries\n",
    "\n",
    "`JSON` (`JavaScript Object Notation`) is a popular data format used for representing structured data. See the chapter in the [Hitchhiker’s Guide to Python](https://docs.python-guide.org/scenarios/json/)\n",
    "\n",
    "The object is a **dictionary**. Dictionaries are Python objects associating keys to values. Keys and Values can be any Python object: scalar, string, list, dictionaries... If a value is a dictionary, then the overall dictionary embed a hierachical structure.\n",
    "\n",
    "See chapter 3.1 of [Python for Data Analysis](https://learning.oreilly.com/library/view/python-for-data/9781491957653/) for more on built_in data structures, including dictionaries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_dict={} # dict are defined by curly braces\n",
    "d1 = {'a' : 'some value', 'b' : [1, 2, 3, 4], 'c' : {'c1': 10, 'c2':20}}\n",
    "d1['d']='more' # add a key-value pair in d1\n",
    "print(d1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#navigating in the dictionary using the keys:\n",
    "print(d1['a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(d1['c']['c2']) # works several time: a handy way to get to an element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tweets[0]._json.keys()) # Keys of dictionary (for the first tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice**  Access the screen name of the first tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### home_timeline\n",
    "Returns the 20 most recent statuses, including retweets, posted by the uthenticating user and that user’s friends. This is the equivalent of /timeline/home on the Web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "public_tweets = api.home_timeline()\n",
    "for tweet in public_tweets:\n",
    "    print(tweet.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### user_timeline\n",
    "The overall rate limit to this method is 100,000 calls during any single 24-hour period. That will translate to 100,000 users and their timeline posts (up to 200 most recent posts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeline = api.user_timeline(user_id=46182536, count=2)\n",
    "print(len(timeline))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods returning a `user` object (or a list of objects)\n",
    "- `me` returns the authenticated user's information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api.me()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The `get_user` method returns information about the specified user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target=account_list[0] #'JenniferWexton'\n",
    "user = api.get_user(target) # argument = id, user_id, screen_name\n",
    "pprint(user._json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some attributes of the `user` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Name:\", user.name)\n",
    "print(\"Screen name:\", user.screen_name)\n",
    "print(\"Number of followers:\" ,  user.followers_count)\n",
    "pprint(\"description: \" + user.description)\n",
    "pprint(\"Number of tweets published: \" + str(user.statuses_count))\n",
    "pprint(\"friends_count: \" + str(user.friends_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `followers` returns the user's followers\n",
    "- `search_users` searches for users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Friendship method: follower_ids\n",
    "This method allows you to get most recent following of a particular user (use screen_name as parameter). \n",
    "It is useful if you want to get all the tweets on the timeline of a particular user. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "followers=api.followers_ids(screen_name=target)\n",
    "print(followers[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch the first 10 tweets published by this account:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = api.user_timeline(screen_name = target, count = 10, include_rts = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looping over `account_list` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling the rate limit imposed by the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "time.sleep(3) # wait for three seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_tweets_by_target=2\n",
    "print(\"We aim at fetching {} tweets\".format(len(account_list)* nb_tweets_by_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "# to get an idea of how long it takes\n",
    "\n",
    "df_tweets=pd.DataFrame() # empty dataframe where the tweet will be saved\n",
    "\n",
    "if len(account_list) > 0:\n",
    "    \n",
    "    # Restricting the search for the first 10 accounts\n",
    "    for target in account_list[:10]:\n",
    "        \n",
    "        # try the following:\n",
    "        try:\n",
    "            # Fetch nb_tweets_by_target for target\n",
    "            tweets = api.user_timeline(screen_name = target, count = nb_tweets_by_target, include_rts = False)\n",
    "            \n",
    "            # Put the tweets into a dataframe object\n",
    "            tweet_count=0\n",
    "            for tweet in tweets:\n",
    "                # 1. Transform the json into a dataframe\n",
    "                df_tweet=pd.DataFrame.from_dict(tweet._json, orient='index', columns=[tweet_count]) # , sleep_on_rate_limit=True\n",
    "                # 2. adds screen name as a row\n",
    "                df_tweet=df_tweet.append(pd.DataFrame({tweet_count:[target]}, \n",
    "                                                      index=['twitter handle']))\n",
    "                # 3. Add the tweet dataframe to the df_tweets dataframe\n",
    "                df_tweets=pd.concat([df_tweet, df_tweets], axis=1)\n",
    "                \n",
    "                # counting the number of target fetched\n",
    "                tweet_count += 1 \n",
    "                \n",
    "            time.sleep(0.5)\n",
    "            \n",
    "        # except if TweepError arises\n",
    "        except tweepy.TweepError: #the error arises when the user has protected tweets\n",
    "            print(\"Failed to run the command on user {}, Skipping...\".format(target))\n",
    "            \n",
    "        # except if RateLimitError arises\n",
    "        except tweepy.RateLimitError:\n",
    "            print(\"ressource usage limit: {} skipped\".format(target))\n",
    "            time.sleep(0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets=df_tweets.transpose() # Transpose the dataset\n",
    "print(df_tweets.columns)\n",
    "print(df_tweets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge tweet and party affiliation on `twitter handle` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_small=df_tweets[['text', 'created_at', 'retweet_count', 'favorite_count', 'twitter handle']] # 'user'\n",
    "df_tweets_small.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged=pd.merge(df_tweets_small, df_merged_all,on='twitter handle')\n",
    "df_merged.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<i class=\"fa fa-warning\"></i>&nbsp;<code>os</code> package\n",
    "    <ul>\n",
    "        <li> <code>os.getcwd()</code>: fetchs the current path\n",
    "        </li>\n",
    "        <li> <code>os.path.dirname()</code>: go back to the parent directory\n",
    "        </li>\n",
    "        <li> <code>os.path.join()</code>: concatenates several paths\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "parent_path=os.path.dirname(os.getcwd()) # os.getcwd() fetchs the current path, \n",
    "data_path=os.path.join(parent_path, 'data')\n",
    "print(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<i class=\"fa fa-warning\"></i>&nbsp;<code>pickle</code> format\n",
    "    <ul>\n",
    "        <li> Useful to store <code>python</code> objects \n",
    "        </li>\n",
    "        <li> Well integrated in  <code>pandas</code> (using <code>to_pickle</code> and <code>read_pickle</code>)\n",
    "        </li>\n",
    "        <li> When the object is not a pandas Dataframe, use the <code>pickle</code> package\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.to_pickle(data_path+'/tweet_labeled.pkl')\n",
    "df_merged.to_csv(data_path+'/tweet_labeled.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other example using API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forecasts from the **Carbon Intensity API**: https://carbonintensity.org.uk/ (include CO2 emissions related to eletricity generation only).\n",
    "See the API [documentation](https://carbon-intensity.github.io/api-definitions/#carbon-intensity-api-v2-0-0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "headers = {\n",
    "  'Accept': 'application/json'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Carbon Intensity data for current half hour\n",
    "r = requests.get('https://api.carbonintensity.org.uk/intensity', params={}, headers = headers) \n",
    "print(r.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Carbon Intensity data for today\n",
    "r = requests.get('https://api.carbonintensity.org.uk/intensity/date', params={}, headers = headers)\n",
    "pprint(r.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Carbon Intensity factors for each fuel type\n",
    "r = requests.get('https://api.carbonintensity.org.uk/intensity/factors', params={}, headers = headers)\n",
    "pprint(r.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Carbon Intensity data for current half hour for GB regions\n",
    "r = requests.get('https://api.carbonintensity.org.uk/regional', params={}, headers = headers)\n",
    "pprint(r.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class survey\n",
    "Please fill in this [short survey](https://framaforms.org/keep-start-stop-survey-1583156515) about the class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is not covered in the notebook\n",
    "\n",
    "- If you struggle something and you need for your project, tell us and we can spend some time on it. For example:\n",
    "    - Scraping dynamically-generated content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "918px",
    "left": "26px",
    "top": "111.567px",
    "width": "192.9px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
